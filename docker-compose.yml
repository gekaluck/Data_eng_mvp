# =============================================================================
# Data Engineering MVP — Local Infrastructure
# =============================================================================
# Services: Postgres (Airflow metadata), MinIO (S3 storage), Airflow (webserver + scheduler)
# Executor: LocalExecutor (no Redis/Celery needed for single-machine learning setup)
# Usage: docker compose up -d
# =============================================================================

# ---------------------------------------------------------------------------
# Shared Airflow configuration (YAML anchor — keeps things DRY)
# ---------------------------------------------------------------------------
x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.4}
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY:?Fernet key must be set in .env}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-apache-airflow-providers-amazon pydantic pyarrow requests pytest}

    # MinIO credentials for DAGs to use
    MINIO_ROOT_USER: ${MINIO_ROOT_USER}
    MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}

    # MinIO S3 connection — set via env var so it's reproducible (no UI clicks needed)
    # Format: aws://<access_key>:<secret_key>@?host=http://minio:9000&extra__aws__endpoint_url=http://minio:9000
    AIRFLOW_CONN_MINIO_S3: aws://${MINIO_ROOT_USER}:${MINIO_ROOT_PASSWORD}@?endpoint_url=http%3A%2F%2Fminio%3A9000

  volumes:
    - ./dags:/opt/airflow/dags
    - airflow-logs:/opt/airflow/logs
    - ./config:/opt/airflow/config
    - ./plugins:/opt/airflow/plugins
    - ./tests:/opt/airflow/tests
  user: "0:0"  # Run as root for Windows compatibility
  depends_on:
    postgres:
      condition: service_healthy

# ---------------------------------------------------------------------------
# Services
# ---------------------------------------------------------------------------
services:

  # -------------------------------------------------------------------------
  # Postgres — Airflow metadata database
  # -------------------------------------------------------------------------
  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # -------------------------------------------------------------------------
  # MinIO — S3-compatible object storage (the "data lake")
  # -------------------------------------------------------------------------
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"    # S3 API
      - "9001:9001"    # Web console
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # -------------------------------------------------------------------------
  # MinIO Init — one-shot: creates bronze/silver/gold buckets
  # Inline entrypoint avoids a separate shell script (no Windows line-ending issues)
  # -------------------------------------------------------------------------
  minio-init:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set local http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&
      mc mb --ignore-existing local/bronze &&
      mc mb --ignore-existing local/silver &&
      mc mb --ignore-existing local/gold &&
      echo 'Buckets created successfully'
      "

  # -------------------------------------------------------------------------
  # Airflow Init — one-shot: run db migrate + create admin user
  # -------------------------------------------------------------------------
  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate &&
        airflow users create \
          --username ${AIRFLOW_ADMIN_USERNAME} \
          --password ${AIRFLOW_ADMIN_PASSWORD} \
          --firstname ${AIRFLOW_ADMIN_FIRSTNAME} \
          --lastname ${AIRFLOW_ADMIN_LASTNAME} \
          --role Admin \
          --email ${AIRFLOW_ADMIN_EMAIL} || true &&
        echo "Airflow init complete"
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'

  # -------------------------------------------------------------------------
  # Airflow Webserver — the UI (http://localhost:8080)
  # -------------------------------------------------------------------------
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # -------------------------------------------------------------------------
  # Airflow Scheduler — picks up and runs DAG tasks
  # With LocalExecutor, the scheduler also executes tasks (no separate worker)
  # -------------------------------------------------------------------------
  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully

# ---------------------------------------------------------------------------
# Named volumes (persist data across restarts)
# ---------------------------------------------------------------------------
volumes:
  postgres-data:
  minio-data:
  airflow-logs:
